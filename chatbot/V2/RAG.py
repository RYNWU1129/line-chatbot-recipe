# # RAG.py
# import os
# import json
# import pandas as pd
# import faiss
# import numpy as np
# from sentence_transformers import SentenceTransformer
# import openai
# from dotenv import load_dotenv
# import firebase_admin
# from firebase_admin import credentials, firestore

# # ----------------------------------------- 
# # ðŸ”¹ åˆå§‹åŒ–å…¨å±€è®Šæ•¸
# # ----------------------------------------- 
# FIREBASE_CRED_PATH = "C:/Users/user/anaconda3/line-chatbot-recipe/chatbot/api1.env.txt"
# CSV_PATH = "C:/Users/user/anaconda3/line-chatbot-recipe/chatbot/RecipeNLG_dataset.csv"
# FAISS_INDEX_PATH = "recipe_faiss.index"
# METADATA_PATH = "recipe_metadata.csv"

# # Load environment variables
# load_dotenv(FIREBASE_CRED_PATH)
# openai_api_key = os.getenv("OPENAI_API_KEY")
# firebase_cred_path = "C:/Users/user/anaconda3/line-chatbot-recipe/chatbot/ai-recipe-87c0b-firebase-adminsdk-fbsvc-1abcfa88d1.json"

# if not openai_api_key:
#     raise ValueError("âŒ OPENAI_API_KEY not found! Check your .env file.")

# # Initialize Firebase
# if not firebase_admin._apps:
#     cred = credentials.Certificate(firebase_cred_path)
#     firebase_admin.initialize_app(cred)
# db = firestore.client()

# # Load embedding model
# model = SentenceTransformer("all-MiniLM-L6-v2")

# # Load recipe dataset and FAISS index
# if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(METADATA_PATH):
#     index = faiss.read_index(FAISS_INDEX_PATH)
#     df = pd.read_csv(METADATA_PATH)
# else:
#     df = pd.read_csv(CSV_PATH, nrows=10000)
#     df["text"] = df.apply(lambda row: f"Title: {row['title']}\nIngredients: {row['ingredients']}\nInstructions: {row['directions']}", axis=1)
#     df["embedding"] = df["text"].apply(lambda x: model.encode(x, convert_to_numpy=True))
#     embeddings = np.vstack(df["embedding"].values)
#     embedding_dim = embeddings.shape[1]
#     index = faiss.IndexFlatL2(embedding_dim)
#     index.add(embeddings)
#     faiss.write_index(index, FAISS_INDEX_PATH)
#     df.to_csv(METADATA_PATH, index=False)

# # ----------------------------------------- 
# # ðŸ”¹ Firestore å‡½æ•¸
# # ----------------------------------------- 
# def get_user_data(user_id):
#     user_ref = db.collection("users").document(user_id)
#     user_doc = user_ref.get()
#     return user_doc.to_dict() if user_doc.exists else None

# def set_user_data(user_id, data):
#     user_ref = db.collection("users").document(user_id)
#     user_ref.set(data, merge=True)

# def get_user_conversation(user_id):
#     user_ref = db.collection("conversations").document(user_id)
#     user_doc = user_ref.get()
#     return user_doc.to_dict().get("messages", []) if user_doc.exists else []

# def save_user_conversation(user_id, messages):
#     user_ref = db.collection("conversations").document(user_id)
#     user_ref.set({"messages": messages}, merge=True)

# # ----------------------------------------- 
# # ðŸ”¹ FAISS æª¢ç´¢
# # ----------------------------------------- 
# def search_recipe(query, k=3):
#     query_embedding = model.encode(query, convert_to_numpy=True).reshape(1, -1)
#     distances, indices = index.search(query_embedding, k)
#     return df.iloc[indices[0]]

# # ----------------------------------------- 
# # ðŸ”¹ GPT æ•´åˆ
# # ----------------------------------------- 
# def chat_with_model(user_id, user_input):
#     user_data = get_user_data(user_id)
#     preferences = user_data.get("preferences", None) if user_data else None

#     if not preferences:
#         return "Please enter your dietary preferences (e.g., vegetarian, no beef, low-carb)."

#     best_recipes = search_recipe(user_input, k=3)
#     formatted_recipes = "\n\n".join([
#         f"**Title:** {row['title']}\n**Ingredients:** {row['ingredients']}\n**Instructions:** {row['directions']}"
#         for _, row in best_recipes.iterrows()
#     ])

#     system_prompt = f"""
#     You are a professional chef assistant. The user follows these dietary preferences: {preferences}.
#     Here are recommended recipes based on their preferences:
#     {formatted_recipes}
#     Provide a response considering these preferences strictly.
#     """

#     conversation = get_user_conversation(user_id)
#     if not conversation:
#         conversation.append({"role": "system", "content": system_prompt})
#     conversation.append({"role": "user", "content": user_input})

#     client = openai.OpenAI(api_key=openai_api_key)
#     response = client.chat.completions.create(
#         model="gpt-3.5-turbo",
#         messages=conversation,
#         max_tokens=200
#     )

#     reply = response.choices[0].message.content
#     conversation.append({"role": "assistant", "content": reply})

#     if len(conversation) > 20:
#         conversation = conversation[-20:]
#     save_user_conversation(user_id, conversation)

#     return reply

import os
import json
import gdown
import pandas as pd
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import openai
import firebase_admin
from firebase_admin import credentials, firestore
import gc

# ----------------------------------------- 
# ðŸ”¹ åˆå§‹åŒ–å…¨å±€è®Šæ•¸
# ----------------------------------------- 
FAISS_INDEX_PATH = "recipe_faiss.index"
METADATA_PATH = "recipe_metadata.csv"
CSV_PATH = "https://drive.google.com/file/d/1IuGWrM_YwnYQwtp06SvWji695NJ7d_wS/view?usp=sharing"

# âœ… å¾žç’°å¢ƒè®Šæ•¸è®€å– API Keys
openai_api_key = os.getenv("OPENAI_API_KEY")
firebase_json = os.getenv("FIREBASE_CREDENTIALS")

if not openai_api_key:
    raise ValueError("âŒ OPENAI_API_KEY not found! Please set it in Render environment variables.")

# âœ… åˆå§‹åŒ– Firebaseï¼ˆå¾žç’°å¢ƒè®Šæ•¸è®€å–ï¼‰
if firebase_json:
    firebase_cred = json.loads(firebase_json)  # è§£æž JSON å­—ä¸²
    if not firebase_admin._apps:  # ç¢ºä¿ Firebase åªåˆå§‹åŒ–ä¸€æ¬¡
        cred = credentials.Certificate(firebase_cred)
        firebase_admin.initialize_app(cred)
else:
    raise ValueError("âŒ FIREBASE_CREDENTIALS not found! Please set it in Render environment variables.")

db = firestore.client()

# âœ… åŠ è¼‰ embedding æ¨¡åž‹
model = SentenceTransformer("all-MiniLM-L6-v2")

# âœ… åˆ†æ‰¹è™•ç†å‡½æ•¸ - æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
def process_csv_in_chunks():
    print("âš ï¸ æœªæ‰¾åˆ°ç¾æœ‰çš„ FAISS ç´¢å¼•å’Œå…ƒæ•¸æ“šï¼Œé–‹å§‹åˆ†æ‰¹è™•ç†è³‡æ–™...")
    
    # å¾ž Google Drive ä¸‹è¼‰æª”æ¡ˆ
    file_id = '1IuGWrM_YwnYQwtp06SvWji695NJ7d_wS'  # å¾ž URL ä¸­æå– ID
    temp_csv_path = 'RecipeNLG_dataset.csv'
    gdown.download(f'https://drive.google.com/uc?id={file_id}', temp_csv_path, quiet=False)
    
    print("âœ… æª”æ¡ˆä¸‹è¼‰å®Œæˆï¼Œé–‹å§‹åˆ†æ‰¹è™•ç†...")
    
    # å‰µå»ºç©ºçš„ FAISS ç´¢å¼•
    # all-MiniLM-L6-v2 çš„åµŒå…¥ç¶­åº¦æ˜¯ 384
    embedding_dim = 384
    index = faiss.IndexFlatL2(embedding_dim)
    
    # å‰µå»ºç©ºçš„å…ƒæ•¸æ“š DataFrame
    metadata_df = pd.DataFrame()
    
    # åˆ†æ‰¹è™•ç† CSV
    chunk_size = 500  # æ¯æ¬¡è™•ç† 500 è¡Œ
    max_rows = 2000   # æ¸›å°‘ç¸½è¡Œæ•¸ä»¥ç¬¦åˆè¨˜æ†¶é«”é™åˆ¶
    rows_processed = 0
    
    # ä½¿ç”¨ chunksize åƒæ•¸åˆ†æ‰¹è®€å– CSV
    for chunk in pd.read_csv(temp_csv_path, chunksize=chunk_size):
        if rows_processed >= max_rows:
            break
            
        # é™åˆ¶æ­¤æ‰¹æ¬¡çš„å¤§å°
        current_chunk = chunk.iloc[:min(chunk_size, max_rows - rows_processed)]
        rows_processed += len(current_chunk)
        
        print(f"ðŸ”„ è™•ç†ç¬¬ {rows_processed-len(current_chunk)+1} è‡³ {rows_processed} è¡Œï¼Œè¨ˆåŠƒè™•ç† {max_rows} è¡Œ")
        
        # å‰µå»ºæ–‡æœ¬å­—æ®µ
        current_chunk["text"] = current_chunk.apply(
            lambda row: f"Title: {row['title']}\nIngredients: {row['ingredients']}\nInstructions: {row['directions']}", 
            axis=1
        )
        
        # ä¿å­˜å…ƒæ•¸æ“šï¼ˆä¸åŒ…å«åµŒå…¥ï¼‰
        metadata_chunk = current_chunk[['title', 'ingredients', 'directions', 'text']]
        
        if metadata_df.empty:
            metadata_df = metadata_chunk.copy()
        else:
            metadata_df = pd.concat([metadata_df, metadata_chunk])
        
        print(f"âœ… é–‹å§‹ç‚ºç¬¬ {rows_processed-len(current_chunk)+1} è‡³ {rows_processed} è¡Œå‰µå»ºåµŒå…¥...")
        
        # åˆ†æ‰¹è™•ç†åµŒå…¥ä»¥æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
        batch_embeddings = []
        for i, row in current_chunk.iterrows():
            # ä¸»å‹•é€²è¡Œåžƒåœ¾å›žæ”¶
            gc.collect()
            
            embedding = model.encode(row["text"], convert_to_numpy=True)
            batch_embeddings.append(embedding)
            
            # æ¯è™•ç† 100 å€‹åµŒå…¥å°±æ¸…ç†ä¸€æ¬¡è¨˜æ†¶é«”
            if (i % 100 == 0) and (i > 0):
                gc.collect()
        
        # æ·»åŠ åˆ° FAISS ç´¢å¼•
        print(f"âœ… æ·»åŠ ç¬¬ {rows_processed-len(current_chunk)+1} è‡³ {rows_processed} è¡Œçš„åµŒå…¥åˆ° FAISS ç´¢å¼•...")
        batch_embeddings_array = np.vstack(batch_embeddings)
        index.add(batch_embeddings_array)
        
        # é‡‹æ”¾è¨˜æ†¶é«”
        del batch_embeddings, batch_embeddings_array, current_chunk, metadata_chunk
        gc.collect()
        print(f"âœ… ç¬¬ {rows_processed} è¡Œè™•ç†å®Œæˆï¼Œé‡‹æ”¾è¨˜æ†¶é«”")
    
    # ä¿å­˜ç´¢å¼•å’Œå…ƒæ•¸æ“š
    print("âœ… ä¿å­˜ FAISS ç´¢å¼•å’Œå…ƒæ•¸æ“š...")
    faiss.write_index(index, FAISS_INDEX_PATH)
    metadata_df.to_csv(METADATA_PATH, index=False)
    
    # æ¸…ç†
    if os.path.exists(temp_csv_path):
        os.remove(temp_csv_path)
    print("âœ… åˆ†æ‰¹è™•ç†å®Œæˆï¼")
    
    return index, metadata_df

# âœ… åŠ è¼‰ FAISS index å’Œé£Ÿè­œæ•¸æ“š
if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(METADATA_PATH):
    print("âœ… æ‰¾åˆ°ç¾æœ‰çš„ FAISS ç´¢å¼•å’Œå…ƒæ•¸æ“šï¼Œç›´æŽ¥è¼‰å…¥")
    index = faiss.read_index(FAISS_INDEX_PATH)
    df = pd.read_csv(METADATA_PATH)
else:
    # ä½¿ç”¨åˆ†æ‰¹è™•ç†ä»£æ›¿åŽŸå§‹è™•ç†æ–¹æ³•
    index, df = process_csv_in_chunks()

# ----------------------------------------- 
# ðŸ”¹ Firestore å‡½æ•¸
# ----------------------------------------- 
def get_user_data(user_id):
    """ å¾ž Firestore ç²å–ä½¿ç”¨è€…æ•¸æ“š """
    user_ref = db.collection("users").document(user_id)
    user_doc = user_ref.get()
    return user_doc.to_dict() if user_doc.exists else None

def set_user_data(user_id, data):
    """ æ›´æ–° Firestore ä¸­çš„ä½¿ç”¨è€…æ•¸æ“š """
    user_ref = db.collection("users").document(user_id)
    user_ref.set(data, merge=True)

def get_user_conversation(user_id):
    """ ç²å–ä½¿ç”¨è€…çš„èŠå¤©è¨˜éŒ„ """
    user_ref = db.collection("conversations").document(user_id)
    user_doc = user_ref.get()
    return user_doc.to_dict().get("messages", []) if user_doc.exists else []

def save_user_conversation(user_id, messages):
    """ å­˜å„²ä½¿ç”¨è€…çš„èŠå¤©è¨˜éŒ„ """
    user_ref = db.collection("conversations").document(user_id)
    user_ref.set({"messages": messages}, merge=True)

# ----------------------------------------- 
# ðŸ”¹ FAISS æª¢ç´¢
# ----------------------------------------- 
def search_recipe(query, k=3):
    """ é€éŽ FAISS æœå°‹ç›¸ä¼¼é£Ÿè­œ """
    try:
        query_embedding = model.encode(query, convert_to_numpy=True).reshape(1, -1)
        distances, indices = index.search(query_embedding, k)
        return df.iloc[indices[0]]
    except Exception as e:
        print(f"âŒ FAISS æª¢ç´¢éŒ¯èª¤: {e}")
        return None

# ----------------------------------------- 
# ðŸ”¹ GPT æ•´åˆ
# ----------------------------------------- 
def chat_with_model(user_id, user_input):
    """ GPT ç”Ÿæˆå›žæ‡‰ä¸¦æ•´åˆ FAISS æœå°‹çµæžœ """
    user_data = get_user_data(user_id)
    preferences = user_data.get("preferences", None) if user_data else None

    if not preferences:
        return "Please enter your dietary preferences (e.g., vegetarian, no beef, low-carb)."

    best_recipes = search_recipe(user_input, k=3)
    if best_recipes is None or best_recipes.empty:
        return "Sorry, I couldn't find any relevant recipes for your request."

    formatted_recipes = "\n\n".join([
        f"**Title:** {row['title']}\n**Ingredients:** {row['ingredients']}\n**Instructions:** {row['directions']}"
        for _, row in best_recipes.iterrows()
    ])

    system_prompt = f"""
    You are a professional chef assistant. The user follows these dietary preferences: {preferences}.
    Here are recommended recipes based on their preferences:
    {formatted_recipes}
    Provide a response considering these preferences strictly.
    """

    conversation = get_user_conversation(user_id)
    if not conversation:
        conversation.append({"role": "system", "content": system_prompt})
    conversation.append({"role": "user", "content": user_input})

    client = openai.OpenAI(api_key=openai_api_key)
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=conversation,
        max_tokens=200
    )

    reply = response.choices[0].message.content
    conversation.append({"role": "assistant", "content": reply})

    if len(conversation) > 20:
        conversation = conversation[-20:]
    save_user_conversation(user_id, conversation)

    return reply